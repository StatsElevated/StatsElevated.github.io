[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Project Gallery",
    "section": "",
    "text": "Informative Missing Imputation Method\n\n\n\n\n\n\nImputation\n\n\nmDAG\n\n\nGaussian Copula\n\n\n\n\n\n\n\n\n\nMay 28, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/main_writeup.html",
    "href": "posts/main_writeup.html",
    "title": "Informative Missing Imputation Method",
    "section": "",
    "text": "&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Traditionally used imputation models for incomplete multivariate data reconstructs the target joint distriubution under a missing-at-random (MAR) mechanism. When the data are missing-not-at-random (MNAR), however, many of these methods become biased: missingness depends on unobserved values, so recovering the target law, \\(p(\\mathbf{X})\\) for a vector of random variables \\(\\mathbf{X}\\) requires modeling the full law, \\(p(\\mathbf{X}, \\mathbf{M})\\) where \\(\\mathbf{M}\\) is a vector of missingness indicators. ======= Last updated: 2025-06-10 &gt;&gt;&gt;&gt;&gt;&gt;&gt; 71fd82e (updated imputation section)\nTraditionally used imputation models for incomplete multivariate data reconstructs the target joint distribution under a missing-at-random (MAR) mechanism. When the data are missing-not-at-random (MNAR), however, many of these methods become biased: missingness depends on unobserved values, so recovering the target law, \\(p(\\mathbf{X})\\) for a vector of random variables \\(\\mathbf{X}\\) requires modeling the full law, \\(p(\\mathbf{X}, \\mathbf{M})\\) where \\(\\mathbf{M}\\) is a vector of missingness indicators."
  },
  {
    "objectID": "posts/main_writeup.html#estimating-tau-under-mnar",
    "href": "posts/main_writeup.html#estimating-tau-under-mnar",
    "title": "Informative Missing Imputation Method",
    "section": "Estimating \\(\\tau\\) Under MNAR",
    "text": "Estimating \\(\\tau\\) Under MNAR\nPrevious work have estimated \\(\\tau\\) by\n\\[\n\\begin{equation}\n\\hat \\tau_{jk} = \\frac{1}{\\binom{n}{2}} \\sum_{i&lt;i'} \\text{sgn}(X_{ji} - X_{ji'})(X_{ki} - X_{ki'})\n\\end{equation}\n\\]\nThis estimator is consistent under missing completely at random setting. Under missing at random or missing not at random setting, the above estimator is biased. In this section, we propose a general framework for constructing an unbiased estimator for \\(\\tau\\) under missing not at random setting with a given missing data directed acyclic graph (mDAG) that characterizes the missingness mechanism.\n\nDefinition 3 Let \\(M\\) be a vector of missingness indicator for \\(X\\) where \\(M_j=1\\) if \\(X_j\\) value is unobserved. conditions\n\nfull law / target law identification criterion - Nabi literature review here. Hypothetical d-separation criteria for our approach to be applicable:\nLet \\(G(X,R)\\) be a mDAG \\(X\\) is a vector of random variables with \\(p\\) dimensions and \\(R\\) is a vector of corresponding missingness indicator.\n\nFor all \\(i\\in\\{1..p\\}\\), we have \\(X_i \\perp_{d-sep} R_i \\mid X_{J_1}, R_{K_1}\\) for some subset \\(X_{J_1}\\subset X\\), and \\(R_{K_1}\\subset R\\) where \\(J_1,\\;K_1\\) are the set of indices.\nIf \\(J_1\\neq\\emptyset\\), then, for each \\(X_j\\in X_{J_1}\\), we have \\(X_j \\perp_{d-sep} R_j\\mid X_{J_2}, R_{K_2}\\).\nThere exists \\(N\\) such that \\(X_{j_N} \\perp_{d-sep} R_{j_N} \\mid X_{J_{N+1}}, R_{K_{N+1}}\\) where \\(J_{N+1}=\\emptyset\\).\n\\(J_1,...,J_N\\) are mutually exclusive."
  },
  {
    "objectID": "posts/main_writeup.html#imputation",
    "href": "posts/main_writeup.html#imputation",
    "title": "Informative Missing Imputation Method",
    "section": "Imputation",
    "text": "Imputation\nOnce we obtain an estimate of \\(\\Sigma\\), we can now sample \\(L\\) from this target distribution or imputation. Namely, we wish to sample from the conditional distribution\n\\[\n\\begin{equation}\nf(X_{miss} \\mid X_{obs}, M) = \\frac{f(X_{miss}, X_{obs}, M)}{\\int\\_{X_{miss}} f(X_{miss}, X_{obs}, M)}=\\frac{f(X_{miss}\\mid X_{obs})f(X_{obs})f(M\\mid X_{pa(M)})f(M)}{\\int_{X_{miss}}f(X_{miss}\\mid X_{obs})f(X_{obs})f(M\\mid X_{pa(M)})f(M)}\n\\end{equation}\n\\tag{1}\\]\nSecond equality holds because there are no directed edges from \\(M\\) to \\(X\\) in an mDAG. Depending on the mDAG structure, \\(f(X)\\) and \\(f(M)\\) can be further factorized. If \\(X_{miss} \\notin X_{pa(M)}\\), then Equation 1 simply becomes \\(\\frac{f(X_{miss}\\mid X_{obs})}{\\int_{X_{miss}}f(X_{miss}\\mid X_{obs})}\\). From the semiparametric Gaussian copula model, using Kendall’s \\(\\tau\\), we have estimated \\(f(L)\\) which is the latent normal distribution underlying \\(X\\). Recall, the cumulative distribution function of \\(X\\), \\(F(X)\\) can be written in terms of \\(L\\) in the following way.\n\\[\n\\begin{align}\nF(x)&=P_X\\begin{pmatrix*}[l]\n    g_c(L_c)&\\leq x_c,\\\\\n    g_t(L_t)\\cdot I( L_t&gt;\\Delta_t)&\\leq x_t,\\\\\n    \\sum_{k=1}^K k\\cdot I(\\Delta_{k-1}&lt;L_o&lt;\\Delta_{k})&\\leq x_o,\\\\\n    I(L_b&gt;\\Delta_b)&\\leq x_b,\\\\\n\\end{pmatrix*}\n=P_L\\begin{pmatrix*}[l]\n    L_c\\leq g_c^{-1}(x_c),\\\\\n    \\begin{cases}\n        L_t\\leq\\Delta_t,\\text{ if } g^{-1}_t(x_t)&lt;\\Delta_t, \\;\\\\\n        L_t\\leq g^{-1}_t(x_t),\\text{ if } g^{-1}_t(x_t)&gt;\\Delta_t,\n    \\end{cases}\\\\\n    L_o\\leq \\Delta_{o,k}, \\text{where } k= x_o\\\\\n    L_b\\leq\\Delta_b, \\text{ if } x_b=0, \\\\\n\\end{pmatrix*}\n\\end{align}\n\\]\nSo, to impute missing values for \\(X_{miss}\\), we sample from the multivariate truncated normal distribution, \\(f(L_{miss}\\mid L_{obs})\\) and apply appropriate transformation with the estimated \\(\\Delta\\)’s and \\(g(\\cdot)\\)’s where \\(g(\\cdot)\\) can be estimated by \\(\\hat g_{c,t}(l_{c,t}) = \\hat F^{-1}(\\Phi(l_{c,t}))\\) where \\(\\hat F\\) is the empirical CDF. as suggested by (Liu, Lafferty, and Wasserman 2009)."
  },
  {
    "objectID": "posts/main_writeup.html#illustrative-example",
    "href": "posts/main_writeup.html#illustrative-example",
    "title": "Informative Missing Imputation Method",
    "section": "Illustrative Example",
    "text": "Illustrative Example\n\nTOIB Study\nThe TOIB study examined whether general practitioners should recommend oral or topical non-steroidal anti-inflammatory drugs (NSAIDs) to older adults with knee pain. A total of 585 study participants were recruited from 26 UK general practices. This study had a comprehensive cohort study design where some practices only enrolled participants to randomized controlled trial (RCT), but some practices gave participants a choice to be randomized, or receive a treatment they preferred (participant preference study, PPS). Knee pain measured by Western Ontario and McMaster Universities Osteoarthritis Index (WOMAC) was the primary outcome and SF-36 was the secondary outcome. Participant demographics were collected at baseline, and WOMAC, SF-12 surveys were filled out at 0, 3,6, 12, and 24 months. Baseline characteristics and baseline survey results were used as covariates, and their relationship is shown in the DAG below.\n\\begin{tikzpicture}[node distance=1.5cm]\n  \\node (X11) [draw, circle] {$X_{11}$};\n  \\node (X12) [draw, circle, below=of X11] {$X_{12}$};\n  \\node (M12) [draw, circle, right=of X12] {$M_{12}$};\n  \\node (M11) [draw, circle, right=of X11] {$M_{11}$};\n\n  \\path[-&gt;]\n    (X11) edge[-] (X12)\n    (X11) edge (M12)\n    (M12) edge[-] (M11);\n\\end{tikzpicture}\nWe can see from the above DAG, that missingness of \\(X_{11}\\) depends on \\(M_{12}\\) and the missingness of \\(X_{12}\\) depends on \\(X_{11}\\). Therefore, we cannot use the observed data to calculate Kendall’s \\(\\tau\\). We show, in Appendix, the following estimation result. \\[\n\\begin{equation}\n\\widehat{\\tau} = \\frac{1}{ \\left( \\begin{array}{c} n \\\\ 2 \\end{array} \\right)} \\sum_{k,k'} \\widehat{h}(m_{12,k},m_{12,k'})\n\\end{equation}\n\\] where\n\\[\n\\begin{equation}\n\\widehat{h(}m_{12},m_{12}') = \\frac{\\sum_{l &lt; l'} \\text{sgn}(X_{11,l} – X_{11,l'}) \\cdot \\widehat{g}(X_{11,l},X_{11,l'}) I( M_{11,l}=0, M_{11,l'}=0, M_{12,l}=m_{12}, M_{12,l'}=m_{12}')   }{\\sum_{l &lt; l'} I(M_{11,l}=0, M_{11,l'}=0, M_{12,l}=m_{12}, M_{12,l'}=m_{12}')}\n\\end{equation}\n\\] \\[\n\\begin{equation}\n\\widehat{g}(\\mathbf{x_{11}}) = \\frac{ \\sum_{j&lt;j'} \\text{sgn}( (X_{12,j} – X_{12,j'}) I(\\mathbf{X_{11}}=\\mathbf{x_{11}}, M_{11,j}=0, M_{11,j'}=0,M_{12,j}=0, M_{12,j'}=0)  }{\\sum_{j&lt;j'}  I(\\mathbf{X_{11}}=\\mathbf{x_{11}},',  M_{11,j}=0, M_{11,j'}=0, M_{12,j}=0, M_{12,j'}=0)}\n\\end{equation}\n\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Stats Elevated",
    "section": "",
    "text": "At StatsElevated, we believe in the power of open collaboration to advance data science and statistical thinking. Our platform is a space where real-world data analysis problems are shared in progress, inviting contributors from diverse backgrounds to engage, collaborate, and build solutions together. By leveraging tools like R Markdown and Quarto, we ensure transparency and reproducibility in every step of the analytical process. Every contribution is tracked and credited, fostering a community that values insight, rigor, and shared learning. Our mission is to elevate statistical problem-solving—together."
  }
]